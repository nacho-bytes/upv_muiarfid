{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9868fdb",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T23:02:34.068262Z",
     "iopub.status.busy": "2023-11-30T23:02:34.068097Z",
     "iopub.status.idle": "2023-11-30T23:02:35.361745Z",
     "shell.execute_reply": "2023-11-30T23:02:35.361127Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch, torchaudio, glob\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def seed_everything(seed):      \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3787945",
   "metadata": {},
   "source": [
    "# Transformer Encoder\n",
    "\n",
    "The transformer encoder is a stack of self-attention and feed-forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3944c413",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T23:02:35.364287Z",
     "iopub.status.busy": "2023-11-30T23:02:35.364024Z",
     "iopub.status.idle": "2023-11-30T23:02:35.388201Z",
     "shell.execute_reply": "2023-11-30T23:02:35.387743Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=1024, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(d_model),\n",
    "            torch.nn.Linear(d_model, d_ff),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b = x.shape[0]\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale       \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, nb_layers=6, seq_len=200, **kwargs):\n",
    "        super().__init__()        \n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, seq_len, kwargs['d_model']))\n",
    "        self.att = torch.nn.ModuleList([SelfAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.ff = torch.nn.ModuleList([FeedForward(**kwargs) for _ in range(nb_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, d = x.shape\n",
    "        x = x + self.pos[:, :t, :]\n",
    "        for att, ff in zip(self.att, self.ff):\n",
    "            x = x + att(x)\n",
    "            x = x + ff(x)            \n",
    "        return x\n",
    "\n",
    "\n",
    "class SpecAug(torch.nn.Module):\n",
    "    def __init__(self, prob_t_warp=0.5,\n",
    "                       t_factor=(0.9, 1.1), \n",
    "                       f_mask_width = (0, 8), \n",
    "                       t_mask_width = (0, 10),\n",
    "                       nb_f_masks=[1,2], \n",
    "                       nb_t_masks=[1,2], \n",
    "                       ):\n",
    "        super().__init__()\n",
    "        self.t_factor = t_factor\n",
    "        self.f_mask_width = f_mask_width\n",
    "        self.t_mask_width = t_mask_width\n",
    "        self.nb_f_masks = nb_f_masks\n",
    "        self.nb_t_masks = nb_t_masks\n",
    "        self.prob_t_warp = prob_t_warp\n",
    "\n",
    "    def time_warp(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, size=(int(x.shape[2]*np.random.uniform(*self.t_factor)), ))\n",
    "        # print('warp', x.shape[2])\n",
    "        return x\n",
    "    \n",
    "    def freq_mask(self, x):\n",
    "        for _ in range(np.random.randint(*self.nb_f_masks)):\n",
    "            f = np.random.randint(*self.f_mask_width)\n",
    "            f0 = np.random.randint(0, x.shape[1]-f)\n",
    "            # print('f', f0, f0+f)\n",
    "            x[:,f0:f0+f,:] = 0\n",
    "        return x\n",
    "\n",
    "    def time_mask(self, x):\n",
    "        for _ in range(np.random.randint(*self.nb_t_masks)):\n",
    "            t = np.random.randint(*self.t_mask_width)\n",
    "            t0 = np.random.randint(0, x.shape[2]-t)\n",
    "            # print('t', t0, t0+t)\n",
    "            x[:,:,t0:t0+t] = 0\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if np.random.uniform() < self.prob_t_warp:\n",
    "            x = self.time_warp(x)\n",
    "        x = self.freq_mask(x)\n",
    "        x = self.time_mask(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb32a4",
   "metadata": {},
   "source": [
    "# Feature Extractor\n",
    "\n",
    "The feature extractor is composed of a log mel-spectrogram and a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19017038",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T23:02:35.389993Z",
     "iopub.status.busy": "2023-11-30T23:02:35.389827Z",
     "iopub.status.idle": "2023-11-30T23:02:35.403926Z",
     "shell.execute_reply": "2023-11-30T23:02:35.403587Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioFeatures(torch.nn.Module):\n",
    "    def __init__(self, feat_dim=80, d_model=512, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fe = torchaudio.transforms.MelSpectrogram(\n",
    "                        n_fft=512, \n",
    "                        win_length=25*16, \n",
    "                        hop_length=10*16, \n",
    "                        n_mels=feat_dim)                            # 25ms window, 10ms shift\n",
    "        # self.spec_aug = SpecAug()\n",
    "        self.linear = torch.nn.Linear(feat_dim, d_model)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.fe(x)\n",
    "        x = (x+1e-6).log()\n",
    "        # if self.training:\n",
    "        #     x = self.spec_aug(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0d219",
   "metadata": {},
   "source": [
    "# Classification network\n",
    "\n",
    "The classification network is composed of an audio feature extractor and a transformer encoder.\n",
    "The prediction is the mean of the transformer encoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70bb5d0",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T23:02:35.405698Z",
     "iopub.status.busy": "2023-11-30T23:02:35.405534Z",
     "iopub.status.idle": "2023-11-30T23:02:35.467127Z",
     "shell.execute_reply": "2023-11-30T23:02:35.466601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "class ClassificationNetwork(torch.nn.Module):\n",
    "    def __init__(self, output_dim, feat_dim=80, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fe = AudioFeatures(**kwargs)\n",
    "        self.encoder = Encoder(**kwargs)\n",
    "        self.norm = torch.nn.LayerNorm(kwargs['d_model'])\n",
    "        self.out = torch.nn.Linear(kwargs['d_model'], output_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.fe(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = x.mean(1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = ClassificationNetwork(output_dim=10, \n",
    "                              d_model=256, \n",
    "                              n_heads=4, \n",
    "                              d_head=32, \n",
    "                              dropout=0.1, \n",
    "                              d_ff=256, \n",
    "                              nb_layers=4)\n",
    "\n",
    "\n",
    "print( model(torch.randn(10, 16000)).shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1ba44",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40542461",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T23:02:35.469442Z",
     "iopub.status.busy": "2023-11-30T23:02:35.469250Z",
     "iopub.status.idle": "2023-11-30T23:02:35.569101Z",
     "shell.execute_reply": "2023-11-30T23:02:35.568682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "class NoiseAug(object):\n",
    "    def __init__(self, noise_dir='musan_small/', prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.noises = glob.glob(noise_dir+'/*/*.wav')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if np.random.uniform() < self.prob:\n",
    "            n = torchaudio.load( np.random.choice(self.noises) )[0][0]            \n",
    "            if len(n) < len(x):\n",
    "                n = torch.nn.functional.pad(n, (0, len(x)-len(n)), value=0)\n",
    "            elif len(n) > len(x):\n",
    "                t0 = np.random.randint(0, len(n) - len(x))\n",
    "                n = n[t0:t0+len(x)]\n",
    "            n = n.numpy()\n",
    "            p_x = x.std()**2\n",
    "            p_n = n.std()**2\n",
    "            snr = np.random.uniform(5, 15)\n",
    "            n = n * np.sqrt(p_x/p_n) * np.power(10, -snr/20)\n",
    "            x = x + n\n",
    "        return x\n",
    "    \n",
    "class RIRAug(object):\n",
    "    def __init__(self, rir_dir='RIRS_NOISES_small/simulated_rirs_small/', prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.rirs = glob.glob(rir_dir+'/*.wav') \n",
    "\n",
    "    def __call__(self, x):\n",
    "        if np.random.uniform() < self.prob:\n",
    "            n = len(x)\n",
    "            rir = torchaudio.load( np.random.choice(self.rirs) )[0][0]\n",
    "            rir = rir.numpy()\n",
    "            rir = rir / np.max(np.abs(rir))\n",
    "            x = scipy.signal.convolve(x, rir)\n",
    "            t0 = np.argmax(np.abs(rir))\n",
    "            x = x[t0:t0+n]\n",
    "        return x\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir='data1/train', audio_len = 16000, transform=[identity]):        \n",
    "        self.transform = transform\n",
    "        self.audio_len = audio_len\n",
    "        self.files = sorted( glob.glob(data_dir+'/*.wav') )        \n",
    "        print(len(self.files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, fs = torchaudio.load(self.files[idx])\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(x, (0, self.audio_len-x.shape[1]), value=0)\n",
    "        \n",
    "        x = x[0].numpy()\n",
    "        for t in self.transform:\n",
    "            x = t(x)\n",
    "\n",
    "        label = self.files[idx].split('.')[-2].split('_')[-1]\n",
    "        # print(x.shape, x.dtype)\n",
    "        return x, int(label)\n",
    "    \n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir='data1/test', audio_len = 16000):\n",
    "        self.audio_len = audio_len       \n",
    "        self.files = sorted(glob.glob(data_dir+'/*.wav'))        \n",
    "        print(len(self.files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, fs = torchaudio.load(self.files[idx])\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(x, (0, self.audio_len-x.shape[1]), value=0)\n",
    "        \n",
    "        x = x[0]\n",
    "        label = self.files[idx].split('.')[-2].split('_')[-1]\n",
    "        # print(x.shape, x.dtype)\n",
    "        return x, int(label)\n",
    "\n",
    "trainset = TrainDataset(transform=[NoiseAug(), RIRAug()])\n",
    "testset = TestDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f61cbb",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be905d4",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T23:02:35.571091Z",
     "iopub.status.busy": "2023-11-30T23:02:35.570943Z",
     "iopub.status.idle": "2023-11-30T23:04:00.015626Z",
     "shell.execute_reply": "2023-11-30T23:04:00.015058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cadrete/software/miniconda3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.4467\n",
      "epoch 1, loss 0.1713\n",
      "epoch 2, loss 0.1386\n",
      "epoch 3, loss 0.1185\n",
      "epoch 4, loss 0.1006\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "nb_epochs = 5\n",
    "batch_size = 32\n",
    "model.train()   \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "for e in range(nb_epochs):\n",
    "    loss_sum = 0\n",
    "    for x, y in trainloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        # print(out.shape, y.shape)\n",
    "        loss = torch.nn.functional.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item() / len(trainloader)\n",
    "    print('epoch %d, loss %.4f' % (e, loss_sum))\n",
    "\n",
    "# torch.save([model, opt], 'model64.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb0a87",
   "metadata": {},
   "source": [
    "# Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31121e26",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T23:04:00.017953Z",
     "iopub.status.busy": "2023-11-30T23:04:00.017687Z",
     "iopub.status.idle": "2023-11-30T23:04:13.279808Z",
     "shell.execute_reply": "2023-11-30T23:04:13.279301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate: 0.0160\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "err = 0\n",
    "for x, y in testset:\n",
    "    x = x.to(device)\n",
    "    \n",
    "    out = model(x[None,...])\n",
    "    y_pred = out.argmax(dim=1).item()\n",
    "    # print(y_pred, y)\n",
    "    if y_pred != y:\n",
    "        err += 1\n",
    "\n",
    "print('error rate: %.4f' % (err/len(testset)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
