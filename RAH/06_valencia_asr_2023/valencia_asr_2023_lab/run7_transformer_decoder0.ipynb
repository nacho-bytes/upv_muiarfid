{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2503b1e6",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:33:02.076445Z",
     "iopub.status.busy": "2023-11-30T22:33:02.076282Z",
     "iopub.status.idle": "2023-11-30T22:33:03.179405Z",
     "shell.execute_reply": "2023-11-30T22:33:03.178772Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch, torchaudio, glob\n",
    "import random\n",
    "import numpy as np  \n",
    "def seed_everything(seed):      \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdabb8",
   "metadata": {},
   "source": [
    "# Transformer Encoder+Decoder\n",
    "\n",
    "The transformer encoder is a stack of self-attention and feed-forward layers.\n",
    "The transformer decoder is a stack of self-attention, cross-attention and feed-forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9fe960",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:33:03.182043Z",
     "iopub.status.busy": "2023-11-30T22:33:03.181781Z",
     "iopub.status.idle": "2023-11-30T22:33:03.219598Z",
     "shell.execute_reply": "2023-11-30T22:33:03.219180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 512])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4972, 0.5028, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3584, 0.2581, 0.3835, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2144, 0.3016, 0.2924, 0.1915, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2935, 0.1486, 0.1699, 0.1692, 0.2187, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1389, 0.3353, 0.1148, 0.1395, 0.1327, 0.1388, 0.0000, 0.0000],\n",
      "        [0.1775, 0.1732, 0.1068, 0.1487, 0.0971, 0.1695, 0.1271, 0.0000],\n",
      "        [0.0929, 0.0962, 0.1774, 0.1276, 0.1229, 0.1352, 0.1250, 0.1228]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 8, 512])\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, seq_len=400, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "        \n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len) == 0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b, n, d = x.shape\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "        \n",
    "        scores = scores.masked_fill(self.mask[:,:,:n,:n], float('-inf'))\n",
    "        att = scores.softmax(dim=-1)\n",
    "        print(att[0,0])\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class CrossAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.norm1(x1)\n",
    "        x2 = self.norm2(x2)  \n",
    "        b = x1.shape[0]\n",
    "        q = self.q_linear(x1).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x2).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x2).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "     \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "model = CrossAttention(d_model=512, n_heads=8, d_head=64, dropout=0.1)\n",
    "x1 = torch.randn(1, 100, 512)\n",
    "x2 = torch.randn(1, 200, 512)\n",
    "print( model(x1, x2).shape )\n",
    "\n",
    "model = CausalSelfAttention(d_model=512, n_heads=8, d_head=64, dropout=0.1, seq_len=8)\n",
    "x = torch.randn(1, 8, 512)\n",
    "print( model(x).shape )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
