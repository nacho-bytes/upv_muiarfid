{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae993c21",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:07:48.184287Z",
     "iopub.status.busy": "2023-11-30T22:07:48.184093Z",
     "iopub.status.idle": "2023-11-30T22:07:49.291375Z",
     "shell.execute_reply": "2023-11-30T22:07:49.290728Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch, torchaudio, glob\n",
    "import random\n",
    "import numpy as np  \n",
    "def seed_everything(seed):      \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ed338",
   "metadata": {},
   "source": [
    "# Transformer Encoder+Decoder\n",
    "\n",
    "The transformer encoder is a stack of self-attention and feed-forward layers.\n",
    "The transformer decoder is a stack of self-attention, cross-attention and feed-forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18409d8f",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:07:49.294237Z",
     "iopub.status.busy": "2023-11-30T22:07:49.293963Z",
     "iopub.status.idle": "2023-11-30T22:07:49.325451Z",
     "shell.execute_reply": "2023-11-30T22:07:49.325115Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=1024, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(d_model),\n",
    "            torch.nn.Linear(d_model, d_ff),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b = x.shape[0]\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "     \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, nb_layers=6, seq_len=400, **kwargs):\n",
    "        super().__init__()        \n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, seq_len, kwargs['d_model']))\n",
    "        self.att = torch.nn.ModuleList([SelfAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.ff = torch.nn.ModuleList([FeedForward(**kwargs) for _ in range(nb_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, d = x.shape\n",
    "        x = x + self.pos[:, :t, :]\n",
    "        for att, ff in zip(self.att, self.ff):\n",
    "            x = x + att(x)\n",
    "            x = x + ff(x)            \n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, seq_len=400, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "        \n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len) == 0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b, n, d = x.shape\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "        \n",
    "        scores = scores.masked_fill(self.mask[:,:,:n,:n], float('-inf'))\n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class CrossAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.norm1(x1)\n",
    "        x2 = self.norm2(x2)  \n",
    "        b = x1.shape[0]\n",
    "        q = self.q_linear(x1).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x2).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x2).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "     \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, nb_layers=6, seq_len=400, **kwargs):\n",
    "        super().__init__()        \n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, seq_len, kwargs['d_model']))\n",
    "        self.att = torch.nn.ModuleList([CausalSelfAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.cross_att = torch.nn.ModuleList([CrossAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.ff = torch.nn.ModuleList([FeedForward(**kwargs) for _ in range(nb_layers)])\n",
    "        \n",
    "    def forward(self, x, enc):\n",
    "        b, t, d = x.shape\n",
    "        x = x + self.pos[:, :t, :]\n",
    "        for att, cross_att, ff in zip(self.att, self.cross_att, self.ff):\n",
    "            x = x + att(x)\n",
    "            x = x + cross_att(x, enc)\n",
    "            x = x + ff(x)            \n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=20, **kwargs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = kwargs['seq_len']\n",
    "        self.emb = torch.nn.Embedding(vocab_size, kwargs['d_model'])\n",
    "        self.enc = Encoder(**kwargs)\n",
    "        self.dec = Decoder(**kwargs)\n",
    "        self.out = torch.nn.Linear(kwargs['d_model'], vocab_size)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = self.emb(x)\n",
    "        return self.enc(x)\n",
    "\n",
    "    def decoder(self, y, enc):\n",
    "        y = self.emb(y)\n",
    "        dec = self.dec(y, enc)\n",
    "        return self.out(dec)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        enc = self.encoder(x)\n",
    "        return self.decoder(y, enc)\n",
    "                \n",
    "    def loss(self, x, y):        \n",
    "        logits = self(x, y[:,:-1])\n",
    "        target = y[:,1:]\n",
    "        loss = torch.nn.functional.cross_entropy(logits.reshape(-1, self.vocab_size), \n",
    "                                                 target.reshape(-1))\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, x):\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        x = torch.nn.functional.pad(torch.tensor(x), (0, self.seq_len-len(x)), value=23)\n",
    "        \n",
    "        y = [20,]\n",
    "        with torch.no_grad():\n",
    "            enc = self.encoder(x.unsqueeze(0).to(device)) \n",
    "            \n",
    "            while y[-1] != 22 and len(y) < self.seq_len:\n",
    "                logits = self.decoder(torch.tensor(y).unsqueeze(0).to(device), enc)\n",
    "                y.append(logits.argmax(-1)[:,-1].item())\n",
    "                \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6320e",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset is a list of 4 integers. \n",
    "The input is the 4 digits and the output is the list of digits and the sum.\n",
    "Additionally, the sum step by step is added to facilitate the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a248c4",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:07:49.327509Z",
     "iopub.status.busy": "2023-11-30T22:07:49.327342Z",
     "iopub.status.idle": "2023-11-30T22:07:49.614009Z",
     "shell.execute_reply": "2023-11-30T22:07:49.613468Z"
    }
   },
   "outputs": [],
   "source": [
    "def num2list(n):\n",
    "    return [int(d) for d in str(n)]\n",
    "\n",
    "\n",
    "class Digitsumset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, digit_len=4, seq_len=20):\n",
    "        super().__init__()\n",
    "        self.digit_len = digit_len\n",
    "        self.seq_len = seq_len\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = num2list(self.data[idx])\n",
    "        sum_x = num2list( sum(x) )\n",
    "        if len(x) == 1:\n",
    "            y = [20, ] + x + [19, ] + x + [21, ] + sum_x + [22]\n",
    "        elif len(x) == 2:\n",
    "            y = [20, ] + x + [19, ] + num2list( sum(x) ) + [21, ] + sum_x + [22]\n",
    "        elif len(x) == 3:\n",
    "            y = [20, ] + x + [19, ] + num2list( x[0] + x[1] ) +  [18] + [x[2]] + [21, ] + sum_x + [22]\n",
    "        else:\n",
    "            y = [20, ] + x + [19, ] + num2list( x[0] + x[1] ) +  [18] + num2list( x[2] + x[3] )+ [21, ] + sum_x + [22]\n",
    "\n",
    "        x = torch.nn.functional.pad(torch.tensor(x), (0, self.seq_len-len(x)), value=23)\n",
    "        y = torch.nn.functional.pad(torch.tensor(y), (0, self.seq_len-len(y)), value=23)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "data = list(range(10000))\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset = Digitsumset(train)\n",
    "testset = Digitsumset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81400833",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297515f7",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:07:49.616381Z",
     "iopub.status.busy": "2023-11-30T22:07:49.616209Z",
     "iopub.status.idle": "2023-11-30T22:10:24.222467Z",
     "shell.execute_reply": "2023-11-30T22:10:24.221971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 7, 7, 9, 5, 10, 1, 5, 13, 10, 14, 5, 13, 14, 5, 14, 5, 13, 10, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cadrete/software/miniconda3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.55\n",
      "epoch 1 loss 0.09\n",
      "epoch 2 loss 0.03\n",
      "epoch 3 loss 0.01\n",
      "epoch 4 loss 0.01\n",
      "epoch 5 loss 0.00\n",
      "epoch 6 loss 0.00\n",
      "epoch 7 loss 0.01\n",
      "epoch 8 loss 0.00\n",
      "epoch 9 loss 0.01\n",
      "epoch 10 loss 0.00\n",
      "epoch 11 loss 0.00\n",
      "epoch 12 loss 0.00\n",
      "epoch 13 loss 0.00\n",
      "epoch 14 loss 0.00\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(vocab_size=24, d_model=128, nb_layers=8, d_ff=256, n_heads=8, d_head=64, dropout=0.1, seq_len=20)\n",
    "print( model.generate([1,2,3,4]) )\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "nb_epochs = 15\n",
    "batch_size = 32\n",
    "model.train()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "for e in range(nb_epochs):\n",
    "    loss_sum = 0\n",
    "    for x, y in trainloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = model.loss(x, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item()\n",
    "    print(f'epoch {e} loss {loss_sum/len(trainloader):.2f}')\n",
    "\n",
    "# torch.save([model, opt], 'model74.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5460cfd",
   "metadata": {},
   "source": [
    "# Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3c40b9",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:10:24.224647Z",
     "iopub.status.busy": "2023-11-30T22:10:24.224425Z",
     "iopub.status.idle": "2023-11-30T22:11:41.453011Z",
     "shell.execute_reply": "2023-11-30T22:11:41.452497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 4, 8, 9, 6, 19, 1, 2, 18, 1, 5, 21, 2, 7, 22]\n",
      "[20, 9, 2, 1, 19, 1, 1, 18, 1, 21, 1, 2, 22]\n",
      "[20, 6, 2, 5, 2, 19, 8, 18, 7, 21, 1, 5, 22]\n",
      "[20, 4, 6, 8, 4, 19, 1, 0, 18, 1, 2, 21, 2, 2, 22]\n",
      "error rate 1.10%,  (11/1000)\n"
     ]
    }
   ],
   "source": [
    "# some train samples\n",
    "print( model.generate([4, 8, 9, 6]) )\n",
    "print( model.generate([9, 2, 1]) )\n",
    "\n",
    "\n",
    "# some test samples\n",
    "print( model.generate([6, 2, 5, 2]) )\n",
    "print( model.generate([4, 6, 8, 4]) )\n",
    "\n",
    "model.eval()\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
    "err = 0\n",
    "for i,(x, y) in enumerate(testloader):\n",
    "    x = x[0].tolist()\n",
    "    y = y[0].tolist()    \n",
    "    y_pred = model.generate(x)    \n",
    "    # print(f'{i}/{len(testloader)}')\n",
    "    # print(x, ' -> ', y)\n",
    "    # print(y_pred)\n",
    "    i = y.index(22)\n",
    "    if str(y[:i + 1]) != str(y_pred):\n",
    "        err += 1\n",
    "print(f'error rate {err/len(testloader):.2%},  ({err}/{len(testloader)})')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
