{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4123f3f4",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:12:55.219437Z",
     "iopub.status.busy": "2023-11-30T22:12:55.218943Z",
     "iopub.status.idle": "2023-11-30T22:12:56.533641Z",
     "shell.execute_reply": "2023-11-30T22:12:56.533122Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch, torchaudio, glob\n",
    "import random\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import editdistance\n",
    "\n",
    "def seed_everything(seed):      \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d2002",
   "metadata": {},
   "source": [
    "# Transformer Encoder+Decoder\n",
    "\n",
    "The transformer encoder is a stack of self-attention and feed-forward layers.\n",
    "The transformer decoder is a stack of self-attention, cross-attention and feed-forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e29f757",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:12:56.536112Z",
     "iopub.status.busy": "2023-11-30T22:12:56.535869Z",
     "iopub.status.idle": "2023-11-30T22:12:56.618146Z",
     "shell.execute_reply": "2023-11-30T22:12:56.617706Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=1024, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(d_model),\n",
    "            torch.nn.Linear(d_model, d_ff),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b = x.shape[0]\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "     \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, nb_layers=6, seq_len=400, **kwargs):\n",
    "        super().__init__()        \n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, seq_len, kwargs['d_model']))\n",
    "        self.att = torch.nn.ModuleList([SelfAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.ff = torch.nn.ModuleList([FeedForward(**kwargs) for _ in range(nb_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, d = x.shape\n",
    "        x = x + self.pos[:, :t, :]\n",
    "        for att, ff in zip(self.att, self.ff):\n",
    "            x = x + att(x)\n",
    "            x = x + ff(x)            \n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, seq_len=400, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "        \n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len) == 0)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        b, n, d = x.shape\n",
    "        q = self.q_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "        \n",
    "        scores = scores.masked_fill(self.mask[:,:,:n,:n], float('-inf'))\n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class CrossAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, d_head=64, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.scale = torch.sqrt(torch.tensor(d_head, dtype=torch.float32))\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_head*n_heads)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(d_head*n_heads, d_model)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.norm1(x1)\n",
    "        x2 = self.norm2(x2)  \n",
    "        b = x1.shape[0]\n",
    "        q = self.q_linear(x1).view(b, -1, self.n_heads, self.d_head)\n",
    "        k = self.k_linear(x2).view(b, -1, self.n_heads, self.d_head)\n",
    "        v = self.v_linear(x2).view(b, -1, self.n_heads, self.d_head) \n",
    "\n",
    "        scores = torch.einsum('bihd,bjhd->bhij', q, k) / self.scale\n",
    "     \n",
    "        att = scores.softmax(dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.einsum('bhij,bjhd->bihd', att, v).reshape(b, -1, self.n_heads*self.d_head)\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "        return out, att\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, nb_layers=6, seq_len=400, **kwargs):\n",
    "        super().__init__()        \n",
    "        self.pos = torch.nn.Parameter(torch.randn(1, seq_len, kwargs['d_model']))\n",
    "        self.att = torch.nn.ModuleList([CausalSelfAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.cross_att = torch.nn.ModuleList([CrossAttention(**kwargs) for _ in range(nb_layers)])\n",
    "        self.ff = torch.nn.ModuleList([FeedForward(**kwargs) for _ in range(nb_layers)])\n",
    "        \n",
    "    def forward(self, x, enc):\n",
    "        b, t, d = x.shape\n",
    "        x = x + self.pos[:, :t, :]\n",
    "        for att, cross_att, ff in zip(self.att, self.cross_att, self.ff):\n",
    "            x = x + att(x)\n",
    "            x = x + cross_att(x, enc)[0]\n",
    "            x = x + ff(x)            \n",
    "        return x\n",
    "\n",
    "class SpecAug(torch.nn.Module):\n",
    "    def __init__(self, prob_t_warp=0.5,\n",
    "                       t_factor=(0.9, 1.1), \n",
    "                       f_mask_width = (0, 8), \n",
    "                       t_mask_width = (0, 10),\n",
    "                       nb_f_masks=[1,2], \n",
    "                       nb_t_masks=[1,2], \n",
    "                       ):\n",
    "        super().__init__()\n",
    "        self.t_factor = t_factor\n",
    "        self.f_mask_width = f_mask_width\n",
    "        self.t_mask_width = t_mask_width\n",
    "        self.nb_f_masks = nb_f_masks\n",
    "        self.nb_t_masks = nb_t_masks\n",
    "        self.prob_t_warp = prob_t_warp\n",
    "\n",
    "    def time_warp(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, size=(int(x.shape[2]*np.random.uniform(*self.t_factor)), ))\n",
    "        # print('warp', x.shape[2])\n",
    "        return x\n",
    "    \n",
    "    def freq_mask(self, x):\n",
    "        for _ in range(np.random.randint(*self.nb_f_masks)):\n",
    "            f = np.random.randint(*self.f_mask_width)\n",
    "            f0 = np.random.randint(0, x.shape[1]-f)\n",
    "            # print('f', f0, f0+f)\n",
    "            x[:,f0:f0+f,:] = 0\n",
    "        return x\n",
    "\n",
    "    def time_mask(self, x):\n",
    "        for _ in range(np.random.randint(*self.nb_t_masks)):\n",
    "            t = np.random.randint(*self.t_mask_width)\n",
    "            t0 = np.random.randint(0, x.shape[2]-t)\n",
    "            # print('t', t0, t0+t)\n",
    "            x[:,:,t0:t0+t] = 0\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if np.random.uniform() < self.prob_t_warp:\n",
    "            x = self.time_warp(x)\n",
    "        x = self.freq_mask(x)\n",
    "        x = self.time_mask(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bbc6f5",
   "metadata": {},
   "source": [
    "# Feature Extractor\n",
    "\n",
    "The feature extractor is composed of a pre-trained wav2vec2 model and a linear layer.\n",
    "The last hidden state of the wav2vec2 model is used as the audio features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b0127a",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:12:56.620290Z",
     "iopub.status.busy": "2023-11-30T22:12:56.620149Z",
     "iopub.status.idle": "2023-11-30T22:12:56.638932Z",
     "shell.execute_reply": "2023-11-30T22:12:56.638528Z"
    }
   },
   "outputs": [],
   "source": [
    "class PretrainedFeatures2(torch.nn.Module):\n",
    "    def __init__(self, freeze=True, d_model=512, **kwargs):\n",
    "        super().__init__()\n",
    "        from transformers import WavLMModel\n",
    "        self.fe = WavLMModel.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-base-plus\")\n",
    "        self.spec_aug = SpecAug()\n",
    "        self.linear = torch.nn.Linear(768, d_model)\n",
    "        if freeze:\n",
    "            for p in self.fe.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x): \n",
    "        with torch.no_grad():\n",
    "            x = self.fe(x).last_hidden_state\n",
    "        # if self.training:\n",
    "        #     x = self.spec_aug(x.transpose(1,2)).transpose(1,2)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class AudioTransformer(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=24, **kwargs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = kwargs['seq_len']\n",
    "        nb_layers = kwargs['nb_layers']\n",
    "        self.fe = PretrainedFeatures2(**kwargs)\n",
    "        kwargs['nb_layers'] = 1\n",
    "        self.enc = Encoder(**kwargs)\n",
    "        kwargs['nb_layers'] = nb_layers\n",
    "        self.emb = torch.nn.Embedding(vocab_size, kwargs['d_model'])\n",
    "        self.dec = Decoder(**kwargs)\n",
    "        self.out = torch.nn.Linear(kwargs['d_model'], vocab_size)\n",
    "        \n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.fe(x)\n",
    "        return self.enc(x)\n",
    "\n",
    "    def decoder(self, y, enc):\n",
    "        y = self.emb(y)\n",
    "        dec = self.dec(y, enc)\n",
    "        return self.out(dec)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        enc = self.encoder(x)\n",
    "        return self.decoder(y, enc)\n",
    "                \n",
    "    def loss(self, x, y):        \n",
    "        logits = self(x, y[:,:-1])\n",
    "        target = y[:,1:]\n",
    "        loss = torch.nn.functional.cross_entropy(logits.reshape(-1, self.vocab_size), \n",
    "                                                 target.reshape(-1))\n",
    "        return loss\n",
    "    \n",
    "           \n",
    "    def generate(self, x):\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        x = torch.nn.functional.pad(torch.tensor(x), (0, self.seq_len-len(x)), value=23)        \n",
    "        if len(x.shape) == 1:\n",
    "            x = x[None, :]\n",
    "        y = [20,]\n",
    "        with torch.no_grad():            \n",
    "            enc = self.encoder(x.to(device))   \n",
    "            \n",
    "            while y[-1] != 22 and len(y) < self.seq_len:\n",
    "                logits = self.decoder(torch.tensor(y).unsqueeze(0).to(device), enc)\n",
    "                y.append(logits.argmax(-1)[:,-1].item())\n",
    "                            \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545384b",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a50eb01",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:12:56.640878Z",
     "iopub.status.busy": "2023-11-30T22:12:56.640629Z",
     "iopub.status.idle": "2023-11-30T22:12:56.677716Z",
     "shell.execute_reply": "2023-11-30T22:12:56.677312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "class NoiseAug(object):\n",
    "    def __init__(self, noise_dir='musan_small/', prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.noises = glob.glob(noise_dir+'/*/*.wav')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if np.random.uniform() < self.prob:\n",
    "            n = torchaudio.load( np.random.choice(self.noises) )[0][0]            \n",
    "            if len(n) < len(x):\n",
    "                n = torch.nn.functional.pad(n, (0, len(x)-len(n)), value=0)\n",
    "            elif len(n) > len(x):\n",
    "                t0 = np.random.randint(0, len(n) - len(x))\n",
    "                n = n[t0:t0+len(x)]\n",
    "            n = n.numpy()\n",
    "            p_x = x.std()**2\n",
    "            p_n = n.std()**2\n",
    "            snr = np.random.uniform(5, 15)\n",
    "            n = n * np.sqrt(p_x/p_n) * np.power(10, -snr/20)\n",
    "            x = x + n\n",
    "        return x\n",
    "    \n",
    "class RIRAug(object):\n",
    "    def __init__(self, rir_dir='RIRS_NOISES_small/simulated_rirs_small/', prob=0.5):\n",
    "        self.prob = prob\n",
    "        self.rirs = glob.glob(rir_dir+'/*.wav') \n",
    "\n",
    "    def __call__(self, x):\n",
    "        if np.random.uniform() < self.prob:\n",
    "            n = len(x)\n",
    "            rir = torchaudio.load( np.random.choice(self.rirs) )[0][0]\n",
    "            rir = rir.numpy()\n",
    "            rir = rir / np.max(np.abs(rir))\n",
    "            x = scipy.signal.convolve(x, rir)\n",
    "            t0 = np.argmax(np.abs(rir))\n",
    "            x = x[t0:t0+n]\n",
    "        return x\n",
    "    \n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir='data3/train', audio_len=4*16000, transform=[identity], seq_len=10):        \n",
    "        self.transform = transform\n",
    "        self.audio_len = audio_len\n",
    "        self.seq_len = seq_len\n",
    "        self.files = sorted( glob.glob(data_dir+'/*.wav') )        \n",
    "        print(len(self.files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, fs = torchaudio.load(self.files[idx])\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(x, (0, self.audio_len-x.shape[1]), value=0)\n",
    "        else:\n",
    "            x = x[:, :self.audio_len]\n",
    "\n",
    "        x = x[0].numpy()\n",
    "        for t in self.transform:\n",
    "            x = t(x)\n",
    "\n",
    "        label = self.files[idx].split('.')[-2].split('_')[-1]\n",
    "        label = label.replace('o', '0')\n",
    "        # print(x.shape, x.dtype)\n",
    "        label = [int(d) for d in str(label)]\n",
    "        y = [20, ] + label + [22, ]\n",
    "        y = torch.nn.functional.pad(torch.tensor(y), (0, self.seq_len-len(y)), value=23)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir='data3/test', audio_len=4*16000, seq_len=10):\n",
    "        self.audio_len = audio_len  \n",
    "        self.seq_len = seq_len     \n",
    "        self.files = sorted(glob.glob(data_dir+'/*.wav'))        \n",
    "        print(len(self.files))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, fs = torchaudio.load(self.files[idx])\n",
    "        if x.shape[1] < self.audio_len:\n",
    "            x = torch.nn.functional.pad(x, (0, self.audio_len-x.shape[1]), value=0)\n",
    "        else:\n",
    "            x = x[:, :self.audio_len]\n",
    "\n",
    "        x = x[0]\n",
    "        label = self.files[idx].split('.')[-2].split('_')[-1]\n",
    "        # print(x.shape, x.dtype)\n",
    "        label = label.replace('o', '0')\n",
    "        label = [int(d) for d in str(label)]\n",
    "        y = [20, ] + label + [22, ]\n",
    "        y = torch.nn.functional.pad(torch.tensor(y), (0, self.seq_len-len(y)), value=23)\n",
    "        return x, y\n",
    "\n",
    "# trainset = TrainDataset(transform=[NoiseAug(), RIRAug()])\n",
    "trainset = TrainDataset()\n",
    "testset = TestDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a98888c",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765ab147",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:12:56.679640Z",
     "iopub.status.busy": "2023-11-30T22:12:56.679500Z",
     "iopub.status.idle": "2023-11-30T22:27:39.994224Z",
     "shell.execute_reply": "2023-11-30T22:27:39.993711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/cadrete/software/miniconda3/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "Some weights of WavLMModel were not initialized from the model checkpoint at patrickvonplaten/wavlm-libri-clean-100h-base-plus and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_12761/1355779910.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.nn.functional.pad(torch.tensor(x), (0, self.seq_len-len(x)), value=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 5, 6, 22]\n",
      "epoch 0 loss 0.61\n",
      "epoch 1 loss 0.07\n",
      "epoch 2 loss 0.03\n",
      "epoch 3 loss 0.02\n",
      "epoch 4 loss 0.02\n",
      "epoch 5 loss 0.02\n",
      "epoch 6 loss 0.02\n",
      "epoch 7 loss 0.01\n",
      "epoch 8 loss 0.01\n",
      "epoch 9 loss 0.01\n",
      "epoch 10 loss 0.01\n",
      "epoch 11 loss 0.01\n",
      "epoch 12 loss 0.01\n",
      "epoch 13 loss 0.01\n",
      "epoch 14 loss 0.01\n",
      "epoch 15 loss 0.01\n",
      "epoch 16 loss 0.01\n",
      "epoch 17 loss 0.01\n",
      "epoch 18 loss 0.01\n",
      "epoch 19 loss 0.01\n"
     ]
    }
   ],
   "source": [
    "model = AudioTransformer(vocab_size=24, d_model=256, nb_layers=4, \n",
    "                         d_ff=512, n_heads=8, d_head=32, dropout=0.1, seq_len=500)\n",
    "\n",
    "print(model.generate( torch.randn(1, 64000 ) ))\n",
    "# input()\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "nb_epochs = 20\n",
    "batch_size = 32\n",
    "model.train()\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "for e in range(nb_epochs):\n",
    "    loss_sum = 0\n",
    "    for x, y in trainloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        opt.zero_grad()\n",
    "        loss = model.loss(x, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item()\n",
    "    print(f'epoch {e} loss {loss_sum/len(trainloader):.2f}')\n",
    "\n",
    "# torch.save([model, opt], 'model83.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc570f82",
   "metadata": {},
   "source": [
    "# Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18358a15",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-11-30T22:27:39.996329Z",
     "iopub.status.busy": "2023-11-30T22:27:39.996108Z",
     "iopub.status.idle": "2023-11-30T22:28:22.996835Z",
     "shell.execute_reply": "2023-11-30T22:28:22.996325Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12761/1355779910.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.nn.functional.pad(torch.tensor(x), (0, self.seq_len-len(x)), value=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate 6.12%,  (301/4919)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "err = 0\n",
    "num = 0\n",
    "for i,(x, y) in enumerate(testset):    \n",
    "    x = x.to(device)    \n",
    "    y_pred = model.generate(x[None,...])\n",
    "\n",
    "    hyp = ' '.join([str(i) for i in y_pred[1:-1]])\n",
    "    y = y.numpy().tolist()\n",
    "    # find the first 22 in list y\n",
    "    y = y[:y.index(22)]\n",
    "    ref = ' '.join([str(i) for i in y[1:]])\n",
    "    # edit distance\n",
    "    # print('(%d/%d)' % (i, len(testset)) )\n",
    "    # print('ref', ref)\n",
    "    # print('hyp', hyp)\n",
    "    \n",
    "    err += editdistance.eval(hyp, ref)\n",
    "    num += len(ref.split())\n",
    "    \n",
    "print(f'error rate {err/num:.2%},  ({err}/{num})')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
